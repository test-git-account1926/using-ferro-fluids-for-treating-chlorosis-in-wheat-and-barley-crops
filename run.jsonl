{"id":"run_1702600000000","experimentId":"exp_transformer_ablation","title":"Self-Attention Head Ablation Study","status":"completed","startDate":"2024-01-20T14:00:00Z","endDate":"2024-01-22T18:30:00Z","codeUrl":"experiments/transformer_ablation_001/run/train.py","dataPath":"data/wmt14_en_de_preprocessed/","hyperparameters":{"model":"transformer_base","num_heads":[1,2,4,8,16],"d_model":512,"num_layers":6,"learning_rate":0.0001,"batch_size":32,"max_epochs":50},"metrics":{"final_bleu":[18.2,22.5,26.8,27.1,27.0],"training_time_hours":[12.5,13.2,14.8,16.3,18.1]},"notes":"Ablation study showing diminishing returns beyond 8 attention heads. 4-8 heads appears to be the sweet spot for this model size.","createdDate":"2024-01-20T13:00:00Z"}
{"id":"run_1702600000001","experimentId":"exp_1702400000000","title":"DistilBERT vs TinyBERT Baseline","status":"running","startDate":"2024-01-25T09:00:00Z","endDate":"","codeUrl":"experiments/efficient_transformer_001/run/baseline.py","dataPath":"data/glue_benchmark/","hyperparameters":{"models":["bert-base","distilbert","tinybert"],"batch_size":32,"eval_batch_size":64,"device":"A100"},"metrics":{"progress":"60%","glue_scores":{"bert-base":{"cola":52.1,"sst2":92.5,"mrpc":88.9},"distilbert":{"cola":48.2,"sst2":91.3,"mrpc":86.5}}},"notes":"Running baseline comparison. TinyBERT evaluation pending. GPU memory usage significantly lower for distilled models.","createdDate":"2024-01-25T08:30:00Z"}