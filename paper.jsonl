{"id":"paper_1702500000000","title":"Attention Is All You Need","authors":"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin","journal":"NeurIPS","year":"2017","doi":"10.5555/3295222.3295349","url":"https://arxiv.org/abs/1706.03762","keyAssumptions":"RNNs and CNNs are necessary for sequence transduction; Sequential computation limits parallelization","citation":"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.","notes":"Foundational paper introducing the Transformer architecture. Key innovation: self-attention mechanism allows modeling dependencies without recurrence or convolution.","addedDate":"2024-01-15T10:00:00Z"}
{"id":"paper_1702500000001","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","authors":"Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova","journal":"NAACL","year":"2019","doi":"10.18653/v1/N19-1423","url":"https://arxiv.org/abs/1810.04805","keyAssumptions":"Unidirectional language models are sufficient for pre-training; Fine-tuning requires task-specific architectures","citation":"Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT (pp. 4171-4186).","notes":"Introduces bidirectional pre-training using masked language modeling. Shows that deep bidirectional representations significantly improve downstream task performance.","addedDate":"2024-01-15T10:05:00Z"}